{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 91451,
          "databundleVersionId": 11223220,
          "sourceType": "competition"
        },
        {
          "sourceId": 237145513,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 237386462,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickyTan8899/tjy/blob/main/%E2%80%9Cpretrain_100epochs%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "DU_i7VmMyZM7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "animal_clef_2025_path = kagglehub.competition_download('animal-clef-2025')\n",
        "hathawaytan_balanced_accuracy_path = kagglehub.notebook_output_download('hathawaytan/balanced-accuracy')\n",
        "hathawaytan_baseline_with_wildfusion_path = kagglehub.notebook_output_download('hathawaytan/baseline-with-wildfusion')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "F9o-C_kayZM9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/WildlifeDatasets/wildlife-datasets@develop\n",
        "!pip install git+https://github.com/WildlifeDatasets/wildlife-tools"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T09:45:16.139653Z",
          "iopub.execute_input": "2025-05-05T09:45:16.139918Z",
          "iopub.status.idle": "2025-05-05T09:45:53.307214Z",
          "shell.execute_reply.started": "2025-05-05T09:45:16.139896Z",
          "shell.execute_reply": "2025-05-05T09:45:53.306339Z"
        },
        "trusted": true,
        "id": "6fsreuNkyZM_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dp9HVwidu8VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Union\n",
        "\n",
        "def baks_compute(\n",
        "        y_true: Union[List, np.ndarray],\n",
        "        y_pred: Union[List, np.ndarray],\n",
        "        identity_test_only: Union[List, np.ndarray]\n",
        "    ) -> float:\n",
        "    \"\"\"Computes BAKS (balanced accuracy on known samples).\n",
        "\n",
        "    Focuses only on samples with known identities (not in identity_test_only).\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        identity_test_only: Labels of unknown identities (only in test set)\n",
        "\n",
        "    Returns:\n",
        "        Balanced accuracy score for known samples\n",
        "    \"\"\"\n",
        "    # Convert inputs to numpy arrays with object dtype to handle mixed types\n",
        "    y_true = np.array(y_true, dtype=object)\n",
        "    y_pred = np.array(y_pred, dtype=object)\n",
        "    identity_test_only = np.array(identity_test_only, dtype=object)\n",
        "\n",
        "    # Filter out unknown samples\n",
        "    mask = ~np.isin(y_true, identity_test_only)\n",
        "    y_true_known = y_true[mask]\n",
        "    y_pred_known = y_pred[mask]\n",
        "\n",
        "    if len(y_true_known) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Get unique classes in the filtered true labels\n",
        "    unique_classes = np.unique(y_true_known)\n",
        "    n_classes = len(unique_classes)\n",
        "\n",
        "    # Compute per-class accuracy and average\n",
        "    class_accuracies = []\n",
        "    for cls in unique_classes:\n",
        "        cls_mask = (y_true_known == cls)\n",
        "        if np.sum(cls_mask) > 0:\n",
        "            cls_acc = np.mean(y_pred_known[cls_mask] == cls)\n",
        "            class_accuracies.append(cls_acc)\n",
        "\n",
        "    # Return the balanced accuracy (mean of per-class accuracies)\n",
        "    return np.mean(class_accuracies) if class_accuracies else 0.0\n",
        "\n",
        "def baus_compute(\n",
        "        y_true: Union[List, np.ndarray],\n",
        "        y_pred: Union[List, np.ndarray],\n",
        "        identity_test_only: Union[List, np.ndarray],\n",
        "        new_class: Union[int, str]\n",
        "    ) -> float:\n",
        "    \"\"\"Computes BAUS (balanced accuracy on unknown samples).\n",
        "\n",
        "    Focuses only on samples with unknown identities (in identity_test_only).\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        identity_test_only: Labels of unknown identities (only in test set)\n",
        "        new_class: Label used for identifying unknown samples\n",
        "\n",
        "    Returns:\n",
        "        Balanced accuracy score for unknown samples\n",
        "    \"\"\"\n",
        "    # Convert inputs to numpy arrays with object dtype to handle mixed types\n",
        "    y_true = np.array(y_true, dtype=object)\n",
        "    y_pred = np.array(y_pred, dtype=object)\n",
        "    identity_test_only = np.array(identity_test_only, dtype=object)\n",
        "\n",
        "    # Filter to include only unknown samples\n",
        "    mask = np.isin(y_true, identity_test_only)\n",
        "    y_true_unknown = y_true[mask]\n",
        "    y_pred_unknown = y_pred[mask]\n",
        "\n",
        "    if len(y_true_unknown) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Get unique unknown classes\n",
        "    unique_unknown_classes = np.unique(y_true_unknown)\n",
        "\n",
        "    # For each unknown class, check if they were correctly predicted as new_class\n",
        "    class_accuracies = []\n",
        "    for cls in unique_unknown_classes:\n",
        "        cls_mask = (y_true_unknown == cls)\n",
        "        if np.sum(cls_mask) > 0:\n",
        "            # For unknown samples, correct prediction is new_class\n",
        "            cls_acc = np.mean(y_pred_unknown[cls_mask] == new_class)\n",
        "            class_accuracies.append(cls_acc)\n",
        "\n",
        "    # Return the balanced accuracy (mean of per-class accuracies)\n",
        "    return np.mean(class_accuracies) if class_accuracies else 0.0\n",
        "\n",
        "\n",
        "def compute_geometric_mean(baks, baus):\n",
        "    return np.sqrt(baks * baus)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T09:45:53.308295Z",
          "iopub.execute_input": "2025-05-05T09:45:53.308546Z",
          "iopub.status.idle": "2025-05-05T09:45:53.318224Z",
          "shell.execute_reply.started": "2025-05-05T09:45:53.308523Z",
          "shell.execute_reply": "2025-05-05T09:45:53.317471Z"
        },
        "id": "7e-rVgTUyZM_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torchvision.transforms as T\n",
        "from wildlife_datasets.datasets import AnimalCLEF2025\n",
        "from wildlife_tools.features import DeepFeatures\n",
        "from wildlife_tools.similarity import CosineSimilarity\n",
        "from wildlife_tools.similarity.wildfusion import SimilarityPipeline, WildFusion\n",
        "from wildlife_tools.similarity.pairwise.lightglue import MatchLightGlue\n",
        "from wildlife_tools.similarity.pairwise.loftr import MatchLOFTR\n",
        "from wildlife_tools.features.local import AlikedExtractor,SuperPointExtractor,SiftExtractor,DiskExtractor\n",
        "from wildlife_tools.similarity.calibration import IsotonicCalibration,LogisticCalibration\n",
        "import sys\n",
        "# sys.path.append('/kaggle/input/balanced-accuracy')  # 添加路径\n",
        "# from metric import score,BAKS,BAUS\n",
        "\n",
        "def create_sample_submission(dataset_query, predictions, file_name='submission.csv'):\n",
        "    df = pd.DataFrame({\n",
        "        'image_id': dataset_query.metadata['image_id'],\n",
        "        'identity': predictions\n",
        "    })\n",
        "    df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-05T09:45:53.31928Z",
          "iopub.execute_input": "2025-05-05T09:45:53.319566Z",
          "iopub.status.idle": "2025-05-05T09:46:18.266933Z",
          "shell.execute_reply.started": "2025-05-05T09:45:53.319543Z",
          "shell.execute_reply": "2025-05-05T09:46:18.266306Z"
        },
        "trusted": true,
        "id": "XRMui0peyZNA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "root = animal_clef_2025_path\n",
        "# transform_display = T.Compose([\n",
        "#     T.Resize([384, 384]),\n",
        "# ])\n",
        "# transform = T.Compose([\n",
        "#     *transform_display.transforms,\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "# ])\n",
        "\n",
        "# transforms_aliked = T.Compose([\n",
        "#     T.Resize([256, 256]),\n",
        "#     T.ToTensor()\n",
        "# ])\n",
        "# transforms_sift = T.Compose([\n",
        "#     T.Resize([512, 512]),\n",
        "#     T.ToTensor()\n",
        "# ])\n",
        "root"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-05T09:46:18.269106Z",
          "iopub.execute_input": "2025-05-05T09:46:18.269657Z",
          "iopub.status.idle": "2025-05-05T09:46:18.274401Z",
          "shell.execute_reply.started": "2025-05-05T09:46:18.269632Z",
          "shell.execute_reply": "2025-05-05T09:46:18.273606Z"
        },
        "trusted": true,
        "id": "OlcrALZ6yZNA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "dataset = AnimalCLEF2025(root, load_label=True,transform=transform)\n",
        "dataset_database = dataset.get_subset(dataset.metadata['split'] == 'database')\n",
        "dataset_query = dataset.get_subset(dataset.metadata['split'] == 'query')\n",
        "dataset_calibration = AnimalCLEF2025(root, df=dataset_database.metadata[:100], load_label=True)\n",
        "meta=dataset.metadata.query('split == \"database\"')\n",
        "num_classes = meta[\"identity\"].nunique()\n",
        "print(f\"种类数: {num_classes}\")\n",
        "n_query=len(dataset_query)\n",
        "n_query"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-05T09:46:18.277298Z",
          "iopub.execute_input": "2025-05-05T09:46:18.277608Z",
          "iopub.status.idle": "2025-05-05T09:46:55.894469Z",
          "shell.execute_reply.started": "2025-05-05T09:46:18.277578Z",
          "shell.execute_reply": "2025-05-05T09:46:55.893485Z"
        },
        "trusted": true,
        "id": "u5p0NoOEyZNA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "import torch\n",
        "import timm\n",
        "import pandas as pd\n",
        "import torchvision.transforms as T\n",
        "from torch.optim import SGD\n",
        "import wildlife_tools\n",
        "from wildlife_tools.data import WildlifeDataset\n",
        "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
        "import importlib\n",
        "import wildlife_tools.data.dataset\n",
        "import gc\n",
        "\n",
        "# 清理 Python 层的垃圾\n",
        "gc.collect()\n",
        "\n",
        "# 清除 CUDA 缓存（释放未使用的显存）\n",
        "torch.cuda.empty_cache()\n",
        "importlib.reload(wildlife_tools.data.dataset)\n",
        "\n",
        "# 使用类\n",
        "WildlifeDataset = wildlife_tools.data.dataset.WildlifeDataset\n",
        "# Dataset configuration\n",
        "# metadata = pd.read_csv('/content/metadata.csv')\n",
        "# image_root = '../data/images/size-256'\n",
        "transform = T.Compose([\n",
        "    T.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
        "    T.RandAugment(num_ops=2, magnitude=20),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "# dataset = WildlifeDataset(\n",
        "#     metadata = metadata.query('split == \"train\"'),\n",
        "#     root = image_root,\n",
        "#     transform=transform\n",
        "# )\n",
        "# train_dataset=AnimalCLEF2025(root=root,transform=transform ,load_label=True)\n",
        "train_dataset=WildlifeDataset(root=root,metadata =meta,transform=transform ,load_label=True)\n",
        "print(len(train_dataset))\n",
        "# Backbone and loss configuration\n",
        "backbone = timm.create_model('swin_base_patch4_window7_224', num_classes=0, pretrained=True)\n",
        "with torch.no_grad():\n",
        "    dummy_input = torch.randn(1, 3, 224, 224)\n",
        "    embedding_size = backbone(dummy_input).shape[1]\n",
        "    print(embedding_size)#1024\n",
        "objective = ArcFaceLoss(num_classes=train_dataset.num_classes, embedding_size=embedding_size, margin=0.5, scale=64)\n",
        "\n",
        "\n",
        "# Optimizer and scheduler configuration\n",
        "params = chain(backbone.parameters(), objective.parameters())\n",
        "optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n",
        "min_lr = optimizer.defaults.get(\"lr\") * 1e-3\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=min_lr)\n",
        "\n",
        "importlib.reload(wildlife_tools.train.trainer)\n",
        "\n",
        "# Step 2: 重新导入类（必须在 reload 之后！）\n",
        "from wildlife_tools.train.trainer import BasicTrainer\n",
        "# Setup training\n",
        "trainer = BasicTrainer(\n",
        "    dataset=train_dataset,\n",
        "    model=backbone,\n",
        "    objective=objective,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    batch_size=64,\n",
        "    accumulation_steps=2,\n",
        "    num_workers=2,\n",
        "    epochs=100,\n",
        "    device='cuda',\n",
        ")\n",
        "\n",
        "trainer.train() # Call the modified train function\n",
        "trainer.save(\"/content/final_checkpoints\", file_name=\"final_model.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9IkgJXHLyZNA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDTpN3M_YACV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S98KQMjZYDh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the models\n",
        "# name = 'hf-hub:BVRA/MegaDescriptor-B-224'\n",
        "# model = timm.create_model(name, num_classes=0, pretrained=True)\n",
        "device = 'cuda'\n",
        "model=backbone\n",
        "pipelines = [\n",
        "\n",
        "    # SimilarityPipeline(\n",
        "    #     matcher = MatchLightGlue(features='superpoint'),\n",
        "    #     extractor = SuperPointExtractor(),\n",
        "    #     transform = T.Compose([\n",
        "    #         T.Resize([512, 512]),\n",
        "    #         T.ToTensor()\n",
        "    #     ]),\n",
        "    #     calibration = IsotonicCalibration()\n",
        "    # ),\n",
        "\n",
        "    SimilarityPipeline(\n",
        "        matcher = MatchLightGlue(features='aliked'),\n",
        "        extractor = AlikedExtractor(),\n",
        "        transform = T.Compose([\n",
        "            T.Resize([256, 256]),\n",
        "            T.ToTensor()\n",
        "        ]),\n",
        "        calibration = IsotonicCalibration()\n",
        "    ),\n",
        "\n",
        "    SimilarityPipeline(\n",
        "        matcher = MatchLightGlue(features='disk'),\n",
        "        extractor = DiskExtractor(),\n",
        "        transform = T.Compose([\n",
        "            T.Resize([256, 256]),\n",
        "            T.ToTensor()\n",
        "        ]),\n",
        "        calibration = IsotonicCalibration()\n",
        "    ),\n",
        "\n",
        "    # SimilarityPipeline(\n",
        "    #     matcher = MatchLightGlue(features='sift'),\n",
        "    #     extractor = SiftExtractor(),\n",
        "    #     transform = T.Compose([\n",
        "    #         T.Resize([512, 512]),\n",
        "    #         T.ToTensor()\n",
        "    #     ]),\n",
        "    #     calibration = IsotonicCalibration()\n",
        "    # ),\n",
        "\n",
        "#     SimilarityPipeline(\n",
        "#         matcher = MatchLOFTR(pretrained='outdoor'),\n",
        "#         extractor = None,\n",
        "#         transform = T.Compose([\n",
        "#             T.Resize([512, 512]),\n",
        "#             T.Grayscale(),\n",
        "#             T.ToTensor(),\n",
        "#         ]),\n",
        "#         calibration = IsotonicCalibration()\n",
        "#     ),\n",
        "\n",
        "#     SimilarityPipeline(\n",
        "#         matcher = CosineSimilarity(),\n",
        "#         extractor = DeepFeatures(\n",
        "#             model = timm.create_model('hf-hub:BVRA/wildlife-mega-L-384', num_classes=0, pretrained=True)\n",
        "#         ),\n",
        "#         transform = T.Compose([\n",
        "#             T.Resize(size=(384, 384)),\n",
        "#             T.ToTensor(),\n",
        "#             T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "#         ]),\n",
        "#         calibration = IsotonicCalibration()\n",
        "#     ),\n",
        "]\n",
        "\n",
        "matcher_mega = SimilarityPipeline(\n",
        "    matcher = CosineSimilarity(),\n",
        "    extractor = DeepFeatures(model=model, device=device, batch_size=16),\n",
        "    transform = transform,\n",
        "    calibration =IsotonicCalibration()\n",
        ")"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T09:46:55.895354Z",
          "iopub.execute_input": "2025-05-05T09:46:55.895619Z",
          "iopub.status.idle": "2025-05-05T09:47:01.752497Z",
          "shell.execute_reply.started": "2025-05-05T09:46:55.895597Z",
          "shell.execute_reply": "2025-05-05T09:47:01.751496Z"
        },
        "trusted": true,
        "id": "0-igJ3euyZNB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibrating the WildFusion\n",
        "\n",
        "wildfusion = WildFusion(calibrated_pipelines = pipelines, priority_pipeline = matcher_mega)\n",
        "wildfusion.fit_calibration(dataset_calibration, dataset_calibration)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T09:47:01.753485Z",
          "iopub.execute_input": "2025-05-05T09:47:01.753781Z",
          "iopub.status.idle": "2025-05-05T09:47:52.625847Z",
          "shell.execute_reply.started": "2025-05-05T09:47:01.753759Z",
          "shell.execute_reply": "2025-05-05T09:47:52.624996Z"
        },
        "id": "sJOnWKARyZNB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute WildFusion similarity\n",
        "similarity = wildfusion(dataset_query, dataset_database, B=25)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T09:47:52.626849Z",
          "iopub.execute_input": "2025-05-05T09:47:52.627137Z",
          "iopub.status.idle": "2025-05-05T09:59:58.125834Z",
          "shell.execute_reply.started": "2025-05-05T09:47:52.627113Z",
          "shell.execute_reply": "2025-05-05T09:59:58.125014Z"
        },
        "id": "_LnftWVMyZNB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pred_idx = similarity.argsort(axis=1)[:,-1]\n",
        "pred_scores = similarity[range(n_query), pred_idx]\n",
        "similarity.shape\n",
        "labels = dataset_database.labels_string\n",
        "pred_scores.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-05T09:59:58.126875Z",
          "iopub.execute_input": "2025-05-05T09:59:58.127213Z",
          "iopub.status.idle": "2025-05-05T09:59:59.537059Z",
          "shell.execute_reply.started": "2025-05-05T09:59:58.12716Z",
          "shell.execute_reply": "2025-05-05T09:59:59.536399Z"
        },
        "trusted": true,
        "id": "FAh0QKvQyZNB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for threshold in [0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8]:\n",
        "    predictions = labels[pred_idx]\n",
        "    predictions[pred_scores < threshold] = 'new_individual'\n",
        "    create_sample_submission(dataset_query, predictions, file_name=f'sample_submission_{threshold}.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-05T09:59:59.537998Z",
          "iopub.execute_input": "2025-05-05T09:59:59.538336Z",
          "iopub.status.idle": "2025-05-05T09:59:59.541819Z",
          "shell.execute_reply.started": "2025-05-05T09:59:59.538304Z",
          "shell.execute_reply": "2025-05-05T09:59:59.541005Z"
        },
        "trusted": true,
        "id": "iL8gf63CyZNB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "gzZl5vnHyZNB"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}